"""
Autonomous Data Science Agent
==============================

This is a truly autonomous system that:
1. Analyzes uploaded datasets
2. Thinks about appropriate analyses  
3. Generates custom code for that specific dataset
4. Saves code to organized folders
5. Executes code and creates visualizations
6. Provides comprehensive summary

Usage:
    python autonomous_data_science_agent.py --file path/to/dataset.csv
"""

import os
import sys
import json
import time
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
import logging

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


@dataclass
class AnalysisPlan:
    """Plan generated by thinking about the dataset"""
    dataset_name: str
    columns: List[str]
    dtypes: Dict[str, str]
    target_variable: str
    task_type: str
    analyses_to_perform: List[str]
    visualizations_to_create: List[str]
    insights_to_find: List[str]
    reasoning: str


class AutonomousDataScienceAgent:
    """
    Truly autonomous agent that analyzes data, thinks, writes code,
    saves it to files, and generates visualizations.
    """
    
    def __init__(self, output_base: str = "autonomous_analysis"):
        """
        Initialize the autonomous agent
        
        Args:
            output_base: Base folder for all outputs
        """
        self.output_base = Path(output_base)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.analysis_history = []
        
    def think_about_dataset(self, df, dataset_path: str) -> AnalysisPlan:
        """
        Agent thinks about the dataset and creates a plan
        """
        logger.info("Thinking about the dataset...")
        
        columns = list(df.columns)
        dtypes = df.dtypes.astype(str).to_dict()
        
        # Analyze column characteristics
        numeric_cols = [c for c in columns if df[c].dtype in ['int64', 'float64']]
        categorical_cols = [c for c in columns if df[c].dtype == 'object']
        
        # Determine target variable
        target_candidates = ['target', 'label', 'class', 'health', 'score', 'outcome']
        target_variable = None
        for col in reversed(columns):
            if any(cand.lower() in col.lower() for cand in target_candidates):
                target_variable = col
                break
        if not target_variable:
            target_variable = columns[-1]
        
        # Determine task type
        if target_variable in categorical_cols:
            task_type = "classification"
        elif target_variable in numeric_cols:
            unique_count = df[target_variable].nunique()
            if unique_count < 20:
                task_type = "classification"
            else:
                task_type = "regression"
        else:
            task_type = "exploratory"
        
        # Build reasoning
        reasoning = f"""
I analyzed the dataset '{Path(dataset_path).stem}' with:
- {len(columns)} columns: {len(numeric_cols)} numeric, {len(categorical_cols)} categorical
- Target variable: '{target_variable}'
- Detected task type: {task_type}

Based on this, I will:
1. Generate descriptive statistics and distributions
2. Create correlation analysis to find relationships
3. Build visualizations to reveal patterns
4. Provide actionable insights specific to this data
        """.strip()
        
        logger.info("Analysis plan created")
        
        return AnalysisPlan(
            dataset_name=Path(dataset_path).stem,
            columns=columns,
            dtypes=dtypes,
            target_variable=target_variable,
            task_type=task_type,
            analyses_to_perform=[],
            visualizations_to_create=[],
            insights_to_find=[],
            reasoning=reasoning
        )
    
    def generate_analysis_code(self, plan: AnalysisPlan, dataset_path: str) -> str:
        """
        Agent generates custom Python code for this specific dataset
        """
        logger.info("Generating custom analysis code...")
        
        columns = plan.columns
        numeric_cols = [c for c in columns if plan.dtypes.get(c, '') in ['int64', 'float64']]
        categorical_cols = [c for c in columns if plan.dtypes.get(c, '') == 'object']
        target = plan.target_variable
        
        # Create code as a list of lines to avoid f-string issues
        code_lines = [
            '"""',
            f'Autonomous Analysis: {plan.dataset_name}',
            f'Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}',
            f'Task Type: {plan.task_type}',
            '',
            'This code was automatically generated by the Autonomous Data Science Agent.',
            '"""',
            '',
            'import pandas as pd',
            'import numpy as np',
            'import matplotlib.pyplot as plt',
            'import seaborn as sns',
            'from datetime import datetime',
            'import json',
            'import warnings',
            'warnings.filterwarnings("ignore")',
            '',
            '# Setup',
            "plt.style.use('seaborn-v0_8')",
            'sns.set_palette("husl")',
            '',
            '# Configuration',
            f'DATASET_PATH = "{dataset_path}"',
            f'timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")',
            '# Use current directory since output folder is already created',
            'OUTPUT_FOLDER = "."',
            '',
            '# Create subdirectories for outputs',
            'import os',
            'os.makedirs("visualizations", exist_ok=True)',
            'os.makedirs("data", exist_ok=True)',
            '',
            '# Load data',
            'print("="*80)',
            'print("AUTONOMOUS DATA ANALYSIS")',
            'print("="*80)',
            'print(f"Dataset: {{{DATASET_PATH}}}")',
            'print(f"Output: {OUTPUT_FOLDER}")',
            'print()',
            '',
            'df = pd.read_csv(DATASET_PATH)',
            "print(f'Dataset loaded: {df.shape[0]} rows x {df.shape[1]} columns')",
            'print()',
            '',
            '# ============================================================',
            '# 1. BASIC STATISTICS',
            '# ============================================================',
            'print("="*80)',
            'print("1. BASIC STATISTICS")',
            'print("="*80)',
            '',
            '# Save basic stats',
            'basic_stats = {',
            f'    "shape": list(df.shape),',
            f'    "columns": list(df.columns),',
            f'    "dtypes": df.dtypes.astype(str).to_dict(),',
            f'    "missing_values": df.isnull().sum().to_dict()',
            '}',
            '',
            'with open(f"{OUTPUT_FOLDER}/data/basic_statistics.json", "w") as f:',
            '    json.dump(basic_stats, f, indent=2, default=str)',
            '',
            'print(df.describe().round(2))',
            'print()',
            'print("Missing values:")',
            'for col in df.columns:',
            '    missing = df[col].isnull().sum()',
            '    if missing > 0:',
            '        print(f"  {col}: {missing}")',
            'print()',
            '',
            '# ============================================================',
            '# 2. DISTRIBUTION ANALYSIS',
            '# ============================================================',
            'print("="*80)',
            'print("2. DISTRIBUTION ANALYSIS")',
            'print("="*80)',
            '',
            '# Distribution plots',
            f'numeric_cols = {numeric_cols}',
            f'categorical_cols = {categorical_cols}',
            '',
            'n_numeric = len(numeric_cols)',
            'n_rows = (n_numeric + 2) // 3',
            'fig, axes = plt.subplots(n_rows, 3, figsize=(15, 4*n_rows))',
            'axes = axes.flatten() if n_rows > 1 else [axes[i] if i < len(axes) else None for i in range(3)]',
            '',
            'for i, col in enumerate(numeric_cols):',
            '    ax = axes[i] if i < len(axes) else None',
            '    if ax is not None:',
            '        df[col].hist(bins=30, ax=ax, edgecolor="black", alpha=0.7)',
            '        ax.set_title(f"{{{col}}} Distribution")',
            '        ax.set_xlabel(col)',
            '        ax.set_ylabel("Frequency")',
            '        mean_val = df[col].mean()',
            '        median_val = df[col].median()',
            '        ax.axvline(mean_val, color="red", linestyle="--", label=f"Mean: {mean_val:.2f}")',
            '        ax.axvline(median_val, color="green", linestyle="--", label=f"Median: {median_val:.2f}")',
            '        ax.legend(fontsize=8)',
            '',
            'for i in range(len(numeric_cols), len(axes)):',
            '    if axes[i] is not None:',
            '        axes[i].axis("off")',
            '',
            'plt.tight_layout()',
            'plt.savefig(f"{OUTPUT_FOLDER}/visualizations/distributions.png", dpi=150, bbox_inches="tight")',
            'plt.close()',
            'print(f"Saved: visualizations/distributions.png")',
            '',
            '# Categorical distributions',
            'if categorical_cols:',
            '    n_cats = len(categorical_cols)',
            '    fig, axes = plt.subplots(1, n_cats, figsize=(5*n_cats, 4))',
            '    axes = [axes] if n_cats == 1 else axes',
            '',
            '    for i, col in enumerate(categorical_cols):',
            '        counts = df[col].value_counts()',
            '        axes[i].pie(counts, labels=counts.index, autopct="%1.1f%%", startangle=90)',
            '        axes[i].set_title(f"{{{col}}} Distribution")',
            '',
            '    plt.tight_layout()',
            'plt.savefig(f"{OUTPUT_FOLDER}/visualizations/categorical_distributions.png", dpi=150, bbox_inches="tight")',
            'plt.close()',
            'print(f"Saved: visualizations/categorical_distributions.png")',
            '',
            '# ============================================================',
            '# 3. CORRELATION ANALYSIS',
            '# ============================================================',
            'print("="*80)',
            'print("3. CORRELATION ANALYSIS")',
            'print("="*80)',
            '',
            'if len(numeric_cols) > 1:',
            '    numeric_data = df[numeric_cols]',
            '    correlation_matrix = numeric_data.corr()',
            '',
            '    # Correlation heatmap',
            '    fig, ax = plt.subplots(figsize=(12, 10))',
            '    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))',
            '    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap="RdYlBu_r",',
            '                center=0, fmt=".2f", square=True, linewidths=0.5, ax=ax,',
            '                cbar_kws={"shrink": 0.8})',
            '    ax.set_title("Correlation Heatmap", fontsize=14, fontweight="bold")',
            '    plt.tight_layout()',
            '    plt.savefig(f"{OUTPUT_FOLDER}/visualizations/correlation_heatmap.png", dpi=150, bbox_inches="tight")',
            '    plt.close()',
            '    print(f"Saved: visualizations/correlation_heatmap.png")',
            '',
            '    # Top correlations',
            '    correlations = []',
            '    for i in range(len(correlation_matrix.columns)):',
            '        for j in range(i):',
            '            corr = correlation_matrix.iloc[i, j]',
            '            col1 = correlation_matrix.columns[i]',
            '            col2 = correlation_matrix.columns[j]',
            '            correlations.append({"pair": f"{col1} vs {col2}", "correlation": corr})',
            '',
            '    correlations.sort(key=lambda x: abs(x["correlation"]), reverse=True)',
            '',
            '    print()',
            '    print("Top 10 Correlations:")',
            '    for item in correlations[:10]:',
            '        sign = "+" if item["correlation"] > 0 else "-"',
            '        corr_val = abs(item["correlation"])',
            "        print(f\"  {item['pair']}: {sign}{corr_val:.3f}\")",
            '',
            '    with open(f"{OUTPUT_FOLDER}/data/correlations.json", "w") as f:',
            '        json.dump(correlations, f, indent=2)',
            '',
            '# ============================================================',
            f'# 4. TARGET ANALYSIS: {target}',
            '# ============================================================',
            'print("="*80)',
            f'print(f"4. TARGET ANALYSIS: {target}")',
            'print("="*80)',
            '',
            f'target = "{target}"',
            '',
            f'if target in numeric_cols:',
            '    fig, axes = plt.subplots(1, 2, figsize=(14, 5))',
            '',
            '    df[target].hist(bins=30, ax=axes[0], edgecolor="black", alpha=0.7, color="steelblue")',
            f'    axes[0].set_title(f"{{target}} Distribution")',
            '    axes[0].set_xlabel(target)',
            '    axes[0].set_ylabel("Frequency")',
            '    axes[0].axvline(df[target].mean(), color="red", linestyle="--", label=f"Mean: {df[target].mean():.2f}")',
            '    axes[0].legend()',
            '',
            '    df.boxplot(column=target, ax=axes[1])',
            f'    axes[1].set_title(f"{{target}} Box Plot")',
            '',
            '    plt.tight_layout()',
            '    plt.savefig(f"{OUTPUT_FOLDER}/visualizations/target_analysis.png", dpi=150, bbox_inches="tight")',
            '    plt.close()',
            '    print(f"Saved: visualizations/target_analysis.png")',
            '',
            f'elif target in categorical_cols:',
            '    df[target].value_counts().plot(kind="bar", ax=plt.gca(), color="steelblue", edgecolor="black")',
            f'    plt.title(f"{{target}} Distribution")',
            '    plt.xlabel(target)',
            '    plt.ylabel("Count")',
            '    plt.xticks(rotation=45)',
            '    plt.tight_layout()',
            '    plt.savefig(f"{OUTPUT_FOLDER}/visualizations/target_analysis.png", dpi=150, bbox_inches="tight")',
            '    plt.close()',
            '    print(f"Saved: visualizations/target_analysis.png")',
            '',
            '# ============================================================',
            '# 5. KEY RELATIONSHIPS VISUALIZATION',
            '# ============================================================',
            'print("="*80)',
            'print("5. KEY RELATIONSHIPS")',
            'print("="*80)',
            '',
            'if len(numeric_cols) >= 2:',
            '    if "correlations" in dir():',
            '        top_pairs = [(c["pair"].split(" vs ")[0], c["pair"].split(" vs ")[1])',
            '                     for c in correlations[:6]]',
            '    else:',
            '        top_pairs = [(numeric_cols[0], col) for col in numeric_cols[1:7]]',
            '',
            '    n_pairs = min(len(top_pairs), 6)',
            '    fig, axes = plt.subplots(2, 3, figsize=(15, 10))',
            '    axes = axes.flatten()',
            '',
            '    for i in range(n_pairs):',
            '        col1, col2 = top_pairs[i]',
            '        if col1 in df.columns and col2 in df.columns:',
            '            ax = axes[i]',
            '            ax.scatter(df[col1], df[col2], alpha=0.5, s=20, c="steelblue")',
            '',
            '            z = np.polyfit(df[col1], df[col2], 1)',
            '            p = np.poly1d(z)',
            '            x_line = np.linspace(df[col1].min(), df[col1].max(), 100)',
            '            ax.plot(x_line, p(x_line), "r--", alpha=0.8, linewidth=2)',
            '',
            '            corr_val = df[col1].corr(df[col2])',
            '            ax.set_title(f"{col1} vs {col2}\\n(r={corr_val:.3f})")',
            '            ax.set_xlabel(col1)',
            '            ax.set_ylabel(col2)',
            '',
            '    plt.tight_layout()',
            '    plt.savefig(f"{OUTPUT_FOLDER}/visualizations/key_relationships.png", dpi=150, bbox_inches="tight")',
            '    plt.close()',
            '    print(f"Saved: visualizations/key_relationships.png")',
            '',
            '# ============================================================',
            '# 6. GROUP ANALYSIS',
            '# ============================================================',
            'print("="*80)',
            'print("6. GROUP ANALYSIS")',
            'print("="*80)',
            '',
            'if categorical_cols:',
            '    for cat_col in categorical_cols[:2]:',
            '        if len(df[cat_col].unique()) <= 10:',
            '            n_nums = min(len(numeric_cols), 4)',
            '            fig, axes = plt.subplots(1, n_nums, figsize=(5*n_nums, 4))',
            '            axes = [axes] if n_nums == 1 else axes',
            '',
            '            for i, num_col in enumerate(numeric_cols[:n_nums]):',
            '                df.boxplot(column=num_col, by=cat_col, ax=axes[i])',
            '                axes[i].set_title(f"{num_col} by {cat_col}")',
            '                axes[i].set_xlabel(cat_col)',
            '',
            '            plt.suptitle(f"Analysis by {cat_col}", fontsize=14, fontweight="bold")',
            '            plt.tight_layout()',
            '            plt.savefig(f"{OUTPUT_FOLDER}/visualizations/group_by_{{cat_col}}.png", dpi=150, bbox_inches="tight")',
            '            plt.close()',
            '            print(f"Saved: visualizations/group_by_{{cat_col}}.png")',
            '',
            '# ============================================================',
            '# 7. SUMMARY REPORT',
            '# ============================================================',
            'print("="*80)',
            'print("7. GENERATING SUMMARY REPORT")',
            'print("="*80)',
            '',
            '# Generate insights',
            'insights = []',
            '',
            'for col in numeric_cols[:5]:',
            '    mean_val = df[col].mean()',
            '    std_val = df[col].std()',
            '    insights.append(f"{col}: Mean={mean_val:.2f}, Std={std_val:.2f}")',
            '',
            'if "correlations" in dir():',
            '    for item in correlations[:5]:',
            '        if abs(item["correlation"]) > 0.3:',
            '            relationship = "positive" if item["correlation"] > 0 else "negative"',
            '            strength = "strong" if abs(item["correlation"]) > 0.6 else "moderate" if abs(item["correlation"]) > 0.4 else "weak"',
            "            insights.append(f\"{item['pair']}: {strength} {relationship} correlation ({item['correlation']:.3f})\")",
            '',
            '# Save summary',
            'summary = {',
            f'    "dataset": {{',
            f'        "name": "{plan.dataset_name}",',
            f'        "rows": int(df.shape[0]),',
            f'        "columns": int(df.shape[1]),',
            f'        "numeric_columns": len(numeric_cols),',
            f'        "categorical_columns": len(categorical_cols)',
            f'    }},',
            "    'insights': insights,",
            '}',
            '',
            'with open(f"{OUTPUT_FOLDER}/data/summary_report.json", "w") as f:',
            '    json.dump(summary, f, indent=2, default=str)',
            '',
            '# Generate markdown report',
            '# Build report content piece by piece to avoid f-string issues',
            "report_lines = [",
            f"    '# Autonomous Analysis Report',",
            f"    '## Dataset: {plan.dataset_name}',",
            f"    'Generated: ' + str(datetime.now())[:19],  # datetime as string",
            "    '',",
            "    '## Overview',",
            "    f'- **Rows:** {df.shape[0]:,}',",
            "    f'- **Columns:** {df.shape[1]}',",
            "    f'- **Numeric columns:** {len(numeric_cols)}',",
            "    f'- **Categorical columns:** {len(categorical_cols)}',",
            "    '',",
            "    '## Key Statistics',",
            "    '',",
            "    '### Distribution Statistics',",
            "    '',",
            "    '## Key Insights',",
            "    '',",
            "    '---',",
            "    '*Generated by Autonomous Data Science Agent*',",
            ']',
            "report_content = '\\n'.join(report_lines)",
            '',
            'with open(f"{OUTPUT_FOLDER}/ANALYSIS_REPORT.md", "w") as f:',
            '    f.write(report_content)',
            '',
            'print(f"Saved: ANALYSIS_REPORT.md")',
            'print(f"Saved: data/summary_report.json")',
            '',
            'print()',
            'print("="*80)',
            'print("ANALYSIS COMPLETE!")',
            'print("="*80)',
            'print(f"All outputs saved to: {OUTPUT_FOLDER}")',
        ]
        
        return '\n'.join(code_lines)
    
    def save_and_execute_code(self, code: str, plan: AnalysisPlan) -> Dict[str, Any]:
        """
        Save code to file and execute it
        """
        logger.info("Saving code to file...")
        
        output_folder = self.output_base / f"output_{plan.dataset_name}_{self.timestamp}"
        code_path = output_folder / f"analysis_{plan.dataset_name}.py"
        
        # Create output directories FIRST before writing the file
        output_folder.mkdir(parents=True, exist_ok=True)
        (output_folder / "visualizations").mkdir(exist_ok=True)
        (output_folder / "data").mkdir(exist_ok=True)
        
        # Now write the code file
        code_path.write_text(code)
        logger.info(f"Code saved: {code_path}")
        
        logger.info("Executing analysis code...")
        
        import subprocess
        # Use just the filename since cwd is set to output_folder
        result = subprocess.run(
            [sys.executable, code_path.name],
            capture_output=True,
            text=True,
            timeout=300,
            cwd=str(output_folder)
        )
        
        execution_result = {
            'success': result.returncode == 0,
            'code_path': str(code_path),
            'output_folder': str(output_folder),
            'stdout': result.stdout,
            'stderr': result.stderr if result.stderr else None
        }
        
        if result.returncode == 0:
            logger.info("Code executed successfully!")
        else:
            logger.error(f"Execution failed: {result.stderr[:500]}")
        
        return execution_result
    
    def analyze(self, dataset_path: str) -> Dict[str, Any]:
        """
        Main entry point - perform complete autonomous analysis
        """
        print("="*80)
        print("AUTONOMOUS DATA SCIENCE AGENT")
        print("="*80)
        
        import pandas as pd
        
        # Step 1: Load and analyze dataset
        print(f"Loading dataset: {dataset_path}")
        df = pd.read_csv(dataset_path)
        print(f"Loaded: {df.shape[0]} rows x {df.shape[1]} columns")
        
        # Step 2: Think about the dataset
        plan = self.think_about_dataset(df, dataset_path)
        print(f"\n{plan.reasoning}")
        
        # Step 3: Generate custom code
        code = self.generate_analysis_code(plan, dataset_path)
        logger.info("Custom code generated")
        
        # Step 4: Save and execute code
        execution_result = self.save_and_execute_code(code, plan)
        
        return {
            'plan': plan,
            'execution': execution_result
        }


def main():
    """Main entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Autonomous Data Science Agent')
    parser.add_argument('--file', '-f', type=str, required=True, help='Path to dataset CSV')
    parser.add_argument('--output', '-o', type=str, default='autonomous_analysis', 
                       help='Output folder base name')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.file):
        print(f"Error: File not found: {args.file}")
        sys.exit(1)
    
    agent = AutonomousDataScienceAgent(output_base=args.output)
    result = agent.analyze(args.file)
    
    return 0 if result['execution'].get('success', False) else 1


if __name__ == "__main__":
    sys.exit(main())
